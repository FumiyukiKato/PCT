{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import geohash\n",
    "import json\n",
    "import collections as cl\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import random\n",
    "import GPencode\n",
    "import TrajectoryHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for reading from csv files\n",
    "\n",
    "def transform_from_dir(dir_name, index, area):\n",
    "    files = glob.glob(\"%s/*\" % dir_name)\n",
    "    for file in files:\n",
    "        extract_columns(file, index, area)\n",
    "\n",
    "# extract necessary columns (time, coordinate) and write to new file.\n",
    "def extract_columns(file_name, index, area):\n",
    "    print(file_name)\n",
    "    time_tokyo = pd.read_csv(file_name, header=None)\n",
    "    time_tokyo = time_tokyo.query('index %% 10 == %s' % index)\n",
    "    time_tokyo = time_tokyo.iloc[:, 3:6]\n",
    "    time_tokyo.columns = [\"time\", \"long\", \"lat\"]\n",
    "    time_tokyo.drop_duplicates()\n",
    "    time_tokyo.to_csv('./data/output-1minute-%s-index-%s.csv' % (area, str(index)), mode='a', index=False, header=False)\n",
    "    \n",
    "\n",
    "# 48bit\n",
    "def encode_batch_data_by_th48(trajectory_data_n, data_list):\n",
    "    trajectory_data_n.columns = [\"time\", \"long\", \"lat\"]\n",
    "    for index, row in tqdm(trajectory_data_n.iterrows()):\n",
    "        encoded_value = TrajectoryHash.encode(\n",
    "            unixepoch_from_str(row[\"time\"]), \n",
    "            TIME_START,\n",
    "            TIME_END,\n",
    "            row[\"lat\"],\n",
    "            row[\"long\"],\n",
    "            theta_t=24,\n",
    "            theta_l=16\n",
    "        )\n",
    "        data_list.append(encoded_value)\n",
    "    return True\n",
    "\n",
    "# 42bit\n",
    "def encode_batch_data_by_th42(trajectory_data_n, data_list):\n",
    "    trajectory_data_n.columns = [\"time\", \"long\", \"lat\"]\n",
    "    for index, row in tqdm(trajectory_data_n.iterrows()):\n",
    "        encoded_value = TrajectoryHash.encode(\n",
    "            unixepoch_from_str(row[\"time\"]), \n",
    "            TIME_START,\n",
    "            TIME_END,\n",
    "            row[\"lat\"],\n",
    "            row[\"long\"],\n",
    "            theta_t=24,\n",
    "            theta_l=14\n",
    "        )\n",
    "        data_list.append(encoded_value)\n",
    "    return True\n",
    "\n",
    "# 36bit\n",
    "def encode_batch_data_by_th36(trajectory_data_n, data_list):\n",
    "    trajectory_data_n.columns = [\"time\", \"long\", \"lat\"]\n",
    "    for index, row in tqdm(trajectory_data_n.iterrows()):\n",
    "        encoded_value = TrajectoryHash.encode(\n",
    "            unixepoch_from_str(row[\"time\"]), \n",
    "            TIME_START,\n",
    "            TIME_END,\n",
    "            row[\"lat\"],\n",
    "            row[\"long\"],\n",
    "            theta_t=24,\n",
    "            theta_l=12\n",
    "        )\n",
    "        data_list.append(encoded_value)\n",
    "    return True\n",
    "\n",
    "# 72bit\n",
    "def encode_batch_data_by_th72(trajectory_data_n, data_list):\n",
    "    trajectory_data_n.columns = [\"time\", \"long\", \"lat\"]\n",
    "    for index, row in tqdm(trajectory_data_n.iterrows()):\n",
    "        encoded_value = TrajectoryHash.encode(\n",
    "            unixepoch_from_str(row[\"time\"]), \n",
    "            TIME_START,\n",
    "            TIME_END,\n",
    "            row[\"lat\"],\n",
    "            row[\"long\"],\n",
    "            theta_t=24,\n",
    "            theta_l=24\n",
    "        )\n",
    "        data_list.append(encoded_value)\n",
    "    return True\n",
    "\n",
    "# 14byte\n",
    "def encode_batch_data_by_gp10(trajectory_data_n, data_list):\n",
    "    trajectory_data_n.columns = [\"time\", \"long\", \"lat\"]\n",
    "    for index, row in tqdm(trajectory_data_n.iterrows()):\n",
    "        encoded_value = GPencode.encode(\n",
    "            unixepoch_from_str(row[\"time\"]), \n",
    "            TIME_START,\n",
    "            TIME_END,\n",
    "            row[\"lat\"],\n",
    "            row[\"long\"],\n",
    "            theta_t=1440,\n",
    "            theta_l=10\n",
    "        )\n",
    "        data_list.append(encoded_value)\n",
    "    return True\n",
    "\n",
    "def unixepoch_from_str(time_str):\n",
    "    # datasetの日付けに2010-10-01などが少数混じっていたので．\n",
    "    modified_time_str = '2008-10-01' + time_str[10:]\n",
    "    return int(datetime.datetime.strptime(modified_time_str, '%Y-%m-%d %H:%M:%S').timestamp())\n",
    "\n",
    "# This is given information here.\n",
    "time_start_str = '2008-10-01 00:00:00'\n",
    "TIME_START = unixepoch_from_str(time_start_str)\n",
    "time_end_str = '2008-10-01 23:59:00'\n",
    "TIME_END = unixepoch_from_str(time_end_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read data and extract necessary columns\n",
    "# for index in range(10):\n",
    "#     for i in range(24):\n",
    "#         transform_from_dir(\"/Users/fumiyuki/Downloads/time-tokyo-%s\" % str(i + 1), index, 'tokyo')\n",
    "#         transform_from_dir(\"/Users/fumiyuki/Downloads/time-kinki-%s\" % str(i + 1), index, 'kinki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e260ca8fb201>:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index, row in tqdm(trajectory_data_n.iterrows()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816a0c6402c44bc5a31882e6ebcb7d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e260ca8fb201>:38: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index, row in tqdm(trajectory_data_n.iterrows()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee54eec8a484dc88511cb3eeea8ab09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e260ca8fb201>:86: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index, row in tqdm(trajectory_data_n.iterrows()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254b8f7c422045049f417bbc91cf99c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af132857682e407f812b305da6ebd5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e0f89fa43a4b3faf3f2f97ae825593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cc9268593a4869b4091a3c363a8033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read and encode trajectories and store data_list\n",
    "\n",
    "th48_data_list = []\n",
    "th42_data_list = []\n",
    "# th36_data_list = []\n",
    "# th72_data_list = []\n",
    "gp10_data_list = []\n",
    "for index in range(1):\n",
    "    for area in [\"tokyo\", \"kinki\"]:\n",
    "        trajectory_data = pd.read_csv('./data/output-1minute-%s-index-%s.csv' % (area, str(index+2)), header=None)\n",
    "        trajectory_data = trajectory_data.drop_duplicates()\n",
    "        encode_batch_data_by_th48(trajectory_data, th48_data_list)\n",
    "        encode_batch_data_by_th42(trajectory_data, th42_data_list)\n",
    "#         encode_batch_data_by_th36(trajectory_data, th36_data_list)\n",
    "#         encode_batch_data_by_th72(trajectory_data, th72_data_list)\n",
    "        encode_batch_data_by_gp10(trajectory_data, gp10_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(gp10_data_list)\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(th48_data_list)\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(th42_data_list)\n",
    "# np.random.seed(seed=0)\n",
    "# np.random.shuffle(th36_data_list)\n",
    "# np.random.seed(seed=0)\n",
    "# np.random.shuffle(th72_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_dump_by_num(data_list, limit_num, method):\n",
    "    tmp_data_list = data_list[:limit_num]\n",
    "    tmp_data_list.sort()\n",
    "    json_data = cl.OrderedDict()\n",
    "    json_data[\"data\"] = tmp_data_list\n",
    "    now_timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = './data/new/%s-central-%s-%s.json' % (method, str(limit_num), now_timestamp)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(json_data, f, indent=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump_by_num(th48_data_list, 10000000, \"th48\")\n",
    "json_dump_by_num(th48_data_list, 20000000, \"th48\")\n",
    "json_dump_by_num(th48_data_list, 30000000, \"th48\")\n",
    "json_dump_by_num(th48_data_list, 40000000, \"th48\")\n",
    "json_dump_by_num(th48_data_list, 50000000, \"th48\")\n",
    "json_dump_by_num(th48_data_list, 70000000, \"th48\")\n",
    "json_dump_by_num(th48_data_list, 100000000, \"th48\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump_by_num(gp10_data_list, 10000000, \"gp10\")\n",
    "json_dump_by_num(gp10_data_list, 20000000, \"gp10\")\n",
    "json_dump_by_num(gp10_data_list, 30000000, \"gp10\")\n",
    "json_dump_by_num(gp10_data_list, 40000000, \"gp10\")\n",
    "json_dump_by_num(gp10_data_list, 50000000, \"gp10\")\n",
    "json_dump_by_num(gp10_data_list, 70000000, \"gp10\")\n",
    "json_dump_by_num(gp10_data_list, 100000000, \"gp10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump_by_num(th42_data_list, 10000000, \"th42\")\n",
    "json_dump_by_num(th42_data_list, 20000000, \"th42\")\n",
    "json_dump_by_num(th42_data_list, 30000000, \"th42\")\n",
    "json_dump_by_num(th42_data_list, 40000000, \"th42\")\n",
    "json_dump_by_num(th42_data_list, 50000000, \"th42\")\n",
    "json_dump_by_num(th42_data_list, 70000000, \"th42\")\n",
    "json_dump_by_num(th42_data_list, 100000000, \"th42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump_by_num(th72_data_list, 10000000, \"th72\")\n",
    "json_dump_by_num(th72_data_list, 20000000, \"th72\")\n",
    "json_dump_by_num(th72_data_list, 30000000, \"th72\")\n",
    "json_dump_by_num(th72_data_list, 40000000, \"th72\")\n",
    "json_dump_by_num(th72_data_list, 50000000, \"th72\")\n",
    "json_dump_by_num(th72_data_list, 70000000, \"th72\")\n",
    "json_dump_by_num(th72_data_list, 100000000, \"th72\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump_by_num(th36_data_list, 10000000, \"th36\")\n",
    "json_dump_by_num(th36_data_list, 20000000, \"th36\")\n",
    "json_dump_by_num(th36_data_list, 30000000, \"th36\")\n",
    "json_dump_by_num(th36_data_list, 40000000, \"th36\")\n",
    "json_dump_by_num(th36_data_list, 50000000, \"th36\")\n",
    "json_dump_by_num(th36_data_list, 70000000, \"th36\")\n",
    "json_dump_by_num(th36_data_list, 100000000, \"th36\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv files and extract necessary columns and distribute for each queries.\n",
    "def transform_from_dir(dir_name, id_data_list, current, batch_size):\n",
    "    files = glob.glob(\"%s/*\" % dir_name)\n",
    "    files.sort()\n",
    "    files = files[current:current+batch_size]\n",
    "    for file in tqdm(files):\n",
    "        id_data = extract_columns(file)\n",
    "        amari = 1440 - len(id_data)\n",
    "        if amari < 0:\n",
    "            if amari != -1440:\n",
    "                print(file)\n",
    "                continue\n",
    "        id_data = id_data.append(id_data.iloc[:amari])\n",
    "        length = len(id_data) // 1440\n",
    "        for i in range(length):\n",
    "            id_data_list.append(id_data[i*1440:(i+1)*1440])\n",
    "\n",
    "\n",
    "def extract_columns(file_name):\n",
    "    id_data = pd.read_csv(file_name, header=None)\n",
    "    id_data = id_data.iloc[:, [3,4,5]]\n",
    "    id_data.columns = [\"time\", \"long\", \"lat\"]\n",
    "    id_data = id_data.drop_duplicates(subset=[\"time\"])\n",
    "    return id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_data_list = []\n",
    "batch_size = 250\n",
    "current = 0\n",
    "for i in range(2):\n",
    "    for j in range(40): # for batch processing\n",
    "        transform_from_dir(\"/Users/fumiyuki/Downloads/tokyo-id-%s/data\" % str(i + 2), id_data_list, current, batch_size)\n",
    "        transform_from_dir(\"/Users/fumiyuki/Downloads/kinki-id-%s\" % str(i + 1), id_data_list, current, batch_size)\n",
    "        current += batch_size\n",
    "    current = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('id_data_list.pickle', 'wb') as f:\n",
    "#     pickle.dump(id_data_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('id_data_list.pickle', 'rb') as f:\n",
    "    data_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_all_data_and_dump_json(data_list, client_limit, method, encode, theta_t, theta_l):\n",
    "    current_id = 0\n",
    "    \n",
    "    json_data = cl.OrderedDict()\n",
    "    same_data = []\n",
    "    total_data_list = []\n",
    "    for i, id_data in tqdm(enumerate(data_list)):\n",
    "        assert(len(id_data) == 1440)\n",
    "        encoded_list = []\n",
    "        for index, row in id_data.iterrows():\n",
    "            encoded_value = encode(\n",
    "                unixepoch_from_str(row[\"time\"]), \n",
    "                TIME_START,\n",
    "                TIME_END,\n",
    "                row[\"lat\"],\n",
    "                row[\"long\"],\n",
    "                theta_t=theta_t,\n",
    "                theta_l=theta_l\n",
    "            )\n",
    "            encoded_list.append(encoded_value)\n",
    "        value = { \"geodata\": encoded_list, \"query_size\": len(id_data), \"query_id\": current_id }\n",
    "        total_data_list.append(value)\n",
    "        current_id += 1\n",
    "        if current_id == client_limit:\n",
    "            break\n",
    "    \n",
    "    json_data[\"data\"] = total_data_list\n",
    "    json_data[\"client_size\"] = current_id\n",
    "    print(\"client size\", current_id)\n",
    "    \n",
    "    now_timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = './data/new/%s-client-%s-%s.json' % (method, str(client_limit), now_timestamp)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(json_data, f, indent=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_all_data_and_dump_json(data_list, 1000, \"th48\", TrajectoryHash.encode, 24, 16)\n",
    "encode_all_data_and_dump_json(data_list, 2000, \"th48\", TrajectoryHash.encode, 24, 16)\n",
    "encode_all_data_and_dump_json(data_list, 3000, \"th48\", TrajectoryHash.encode, 24, 16)\n",
    "encode_all_data_and_dump_json(data_list, 4000, \"th48\", TrajectoryHash.encode, 24, 16)\n",
    "encode_all_data_and_dump_json(data_list, 5000, \"th48\", TrajectoryHash.encode, 24, 16)\n",
    "encode_all_data_and_dump_json(data_list, 6000, \"th48\", TrajectoryHash.encode, 24, 16)\n",
    "encode_all_data_and_dump_json(data_list, 7000, \"th48\", TrajectoryHash.encode, 24, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-f0fc7dc4e3db>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, id_data in tqdm(enumerate(data_list)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdaf8cbdd8634813ac1deb986c01761e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "client size 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd2ba9d55154af694f43615000e22c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "client size 2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1132317fbc9c4659a10e69ffcda6d5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "client size 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc94e3ab4a3439baf563132757c82d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "client size 4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5ceae0a0c34b4d84de41a40630b8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "client size 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1e130f5a4346f6ac08bc9db353c0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "client size 6000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ea1806103548d49f25c0f2e64931ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "client size 7000\n"
     ]
    }
   ],
   "source": [
    "encode_all_data_and_dump_json(data_list, 1000, \"gp10\", GPencode.encode, 1440, 10)\n",
    "encode_all_data_and_dump_json(data_list, 2000, \"gp10\", GPencode.encode, 1440, 10)\n",
    "encode_all_data_and_dump_json(data_list, 3000, \"gp10\", GPencode.encode, 1440, 10)\n",
    "encode_all_data_and_dump_json(data_list, 4000, \"gp10\", GPencode.encode, 1440, 10)\n",
    "encode_all_data_and_dump_json(data_list, 5000, \"gp10\", GPencode.encode, 1440, 10)\n",
    "encode_all_data_and_dump_json(data_list, 6000, \"gp10\", GPencode.encode, 1440, 10)\n",
    "encode_all_data_and_dump_json(data_list, 7000, \"gp10\", GPencode.encode, 1440, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
