{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import geohash\n",
    "import json\n",
    "import collections as cl\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import random\n",
    "import GPencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## common encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent to periodical encoding\n",
    "def timehash_encode(time):\n",
    "    return str(int(time[11:13])*6 + int(time[14])).zfill(4)\n",
    "\n",
    "# periodical encoding\n",
    "def timehash_encode_for_1minute(time):\n",
    "    return str(int(time[11:13])*60 + int(time[14:16])).zfill(4)\n",
    "\n",
    "def encode(time, latitude, longtitude):\n",
    "    t_hash = timehash_encode_for_1minute(time)\n",
    "    g_hash = geohash.encode(latitude, longtitude, 10)\n",
    "    return g_hash + t_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for reading from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_from_dir(dir_name, index, area):\n",
    "    files = glob.glob(\"%s/*\" % dir_name)\n",
    "    for file in files:\n",
    "        extract_columns(file, index, area)\n",
    "\n",
    "# extract necessary columns (time, coordinate) and write to new file.\n",
    "def extract_columns(file_name, index, area):\n",
    "    print(file_name)\n",
    "    time_tokyo = pd.read_csv(file_name, header=None)\n",
    "    time_tokyo = time_tokyo.query('index %% 10 == %s' % index)\n",
    "    time_tokyo = time_tokyo.iloc[:, 3:6]\n",
    "    time_tokyo.columns = [\"time\", \"long\", \"lat\"]\n",
    "    time_tokyo.drop_duplicates()\n",
    "    time_tokyo.to_csv('./data/output-1minute-%s-index-%s.csv' % (area, str(index)), mode='a', index=False, header=False)\n",
    "    \n",
    "\n",
    "# execute encode\n",
    "def encode_batch_data(trajectory_data_n, data_list):\n",
    "    trajectory_data_n.columns = [\"time\", \"long\", \"lat\"]\n",
    "    for index, row in tqdm(trajectory_data_n.iterrows()):\n",
    "        encoded_value = encode(row[\"time\"], row[\"lat\"], row[\"long\"])\n",
    "        data_list.append(encoded_value)\n",
    "    return True\n",
    "\n",
    "\n",
    "# execute encode\n",
    "# GPencode parameter theta_t = 1440, theta_l = 10\n",
    "def encode_batch_data(trajectory_data_n, data_list):\n",
    "    trajectory_data_n.columns = [\"time\", \"long\", \"lat\"]\n",
    "    for index, row in tqdm(trajectory_data_n.iterrows()):\n",
    "        encoded_value = GPencode.encode(\n",
    "            unixepoch_from_str(row[\"time\"]), \n",
    "            TIME_START,\n",
    "            TIME_END,\n",
    "            row[\"lat\"],\n",
    "            row[\"long\"],\n",
    "            theta_t=1440,\n",
    "            theta_l=10\n",
    "        )\n",
    "        data_list.append(encoded_value)\n",
    "    return True\n",
    "\n",
    "def unixepoch_from_str(time_str):\n",
    "    return datetime.datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# This is given information here.\n",
    "time_start_str = '2008-10-01 00:00:00'\n",
    "TIME_START = unixepoch_from_str(time_start_str)\n",
    "time_end_str = '2008-10-01 23:59:00'\n",
    "TIME_END = unixepoch_from_str(time_end_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ファイル名とデータセットについて\n",
    "\n",
    "/Users/fumiyuki/Downloads/time-tokyo-%s\n",
    "    東京近辺における軌跡データのオリジナルのデータセット\n",
    "/Users/fumiyuki/Downloads/time-kinki-%s\n",
    "    近畿近辺における軌跡データのオリジナルのデータセット\n",
    "\n",
    "\n",
    "    2008-10-01 00:00:00　から2008-10-01 23:59:00 まで1分ごとにデータが入っている．\n",
    "    同時刻に何万ものデータが入っているが，データは匿名化されていいて，このデータセットから一人あたりのデータセットを再現することはできない．\n",
    "    なのでサーバサイドのデータとして利用する\n",
    "    \n",
    "    \n",
    "    一方で\n",
    "/Users/fumiyuki/Downloads/tokyo-id-%s\n",
    "/Users/fumiyuki/Downloads/kinki-id-%s\n",
    "    これらは，idごとに区別可能な1日分のデータセットがある．\n",
    "    これらからクライアントのデータセットを作成する．ただしデータは1分ごとであり，1idあたり1日分しか識別できない．\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read data and extract necessary columns\n",
    "for index in range(10):\n",
    "    for i in range(24):\n",
    "        transform_from_dir(\"/Users/fumiyuki/Downloads/time-tokyo-%s\" % str(i + 1), index, 'tokyo')\n",
    "        transform_from_dir(\"/Users/fumiyuki/Downloads/time-kinki-%s\" % str(i + 1), index, 'kinki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "./data/output-1minute-%s-index-%s.csv\n",
    "    元のデータから必要なカラムを抜き取って保存してあるcsvファイル．元のデータのサイズが大きくて使いづらいので小さくしている．\n",
    "    あと，元のデータは時間ごとに分けられているため，\n",
    "    時間の粒度は1分ごとで人の区別はない．サーバサイドのデータとして扱うので人を区別する必要はない．\n",
    "    同じ時間，同じ場所のデータは落としているので注意． \n",
    "    時間空間的に公平になるようにTokyo, Kinkiのそれぞのデータセットから 　mod index でindexの剰余のデータごとに10個に分けている．\n",
    "    なので各データセットに時間が均等に分布している．\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read and encode trajectories and store data_list\n",
    "data_list = []\n",
    "for index in range(1):\n",
    "    for area in [\"tokyo\", \"kinki\"]:\n",
    "        trajectory_data = pd.read_csv('./data/output-1minute-%s-index-%s.csv' % (area, str(index+1)), header=None)\n",
    "        trajectory_data = trajectory_data.drop_duplicates()\n",
    "        encode_batch_data(trajectory_data, data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache as pickle\n",
    "\n",
    "\"\"\"\n",
    "index1-2.pickleは上のindexが1と2のものをまとめたもので，\n",
    "中には，東京都近畿のデータセットのうちの20%が含まれていることになる．\n",
    "\"\"\"\n",
    "# with open('index-1-2.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('index-1.pickle', 'rb') as f:\n",
    "#     data_list = pickle.load(f)\n",
    "\n",
    "# with open('index-1-2.pickle', 'rb') as f:\n",
    "#     data_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write server side infected data\n",
    "\n",
    "\"\"\" format\n",
    "{\n",
    "    \"data\": [\n",
    "        \"xn7ehpxnex0001\",\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_dump_by_num(data_list, limit_num):\n",
    "    tmp_data_list = data_list[:limit_num]\n",
    "    tmp_data_list.sort()\n",
    "    json_data = cl.OrderedDict()\n",
    "    json_data[\"data\"] = tmp_data_list\n",
    "    now_timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = './data/central-1minute-%s-%s.json' % (str(limit_num), now_timestamp)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(json_data, f, indent=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_list)\n",
    "json_dump_by_num(data_list, 5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## client side query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv files and extract necessary columns and distribute for each queries.\n",
    "def transform_from_dir(dir_name, id_data_list, current, batch_size):\n",
    "    files = glob.glob(\"%s/*\" % dir_name)\n",
    "    files.sort()\n",
    "    files = files[current:current+batch_size]\n",
    "    for file in tqdm(files):\n",
    "        id_data = extract_columns(file)\n",
    "        amari = 1440 - len(id_data)\n",
    "        if amari < 0:\n",
    "            if amari != -1440:\n",
    "                print(file)\n",
    "                continue\n",
    "        id_data = id_data.append(id_data.iloc[:amari])\n",
    "        length = len(id_data) // 1440\n",
    "        for i in range(length):\n",
    "            id_data_list.append(id_data[i*1440:(i+1)*1440])\n",
    "\n",
    "\n",
    "def extract_columns(file_name):\n",
    "    id_data = pd.read_csv(file_name, header=None)\n",
    "    id_data = id_data.iloc[:, [3,4,5]]\n",
    "    id_data.columns = [\"time\", \"long\", \"lat\"]\n",
    "    id_data = id_data.drop_duplicates(subset=[\"time\"])\n",
    "    return id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_data_list = []\n",
    "batch_size = 250\n",
    "current = 0\n",
    "for i in range(2):\n",
    "    for j in range(40): # for batch processing\n",
    "        transform_from_dir(\"/Users/fumiyuki/Downloads/tokyo-id-%s/data\" % str(i + 2), id_data_list, current, batch_size)\n",
    "        transform_from_dir(\"/Users/fumiyuki/Downloads/kinki-id-%s\" % str(i + 1), id_data_list, current, batch_size)\n",
    "        current += batch_size\n",
    "    current = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('id_data_list.pickle', 'wb') as f:\n",
    "#     pickle.dump(id_data_list, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('id_data_list.pickle', 'rb') as f:\n",
    "    data_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_all_data_and_dump_json(data_list, client_limit):\n",
    "    current_id = 0\n",
    "    \n",
    "    json_data = cl.OrderedDict()\n",
    "    same_data = []\n",
    "    total_data_list = []\n",
    "    for i, id_data in tqdm(enumerate(data_list)):\n",
    "        assert(len(id_data) == 1440)\n",
    "        encoded_list = []\n",
    "        for index, row in id_data.iterrows():\n",
    "            encoded_value =GPencode.encode(\n",
    "                unixepoch_from_str(row[\"time\"]), \n",
    "                TIME_START,\n",
    "                TIME_END,\n",
    "                row[\"lat\"],\n",
    "                row[\"long\"],\n",
    "                theta_t=1440,\n",
    "                theta_l=10\n",
    "            )\n",
    "            encoded_list.append(encoded_value)\n",
    "        value = { \"geodata\": encode_list, \"query_size\": len(id_data), \"query_id\": current_id }\n",
    "        total_data_list.append(value)\n",
    "        current_id += 1\n",
    "        if current_id == client_limit:\n",
    "            break\n",
    "    \n",
    "    json_data[\"data\"] = total_data_list\n",
    "    json_data[\"client_size\"] = current_id\n",
    "    print(\"client size\", current_id)\n",
    "    \n",
    "    now_timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = './data/geohash10-client-%s-real-for-1minute-%s.json' % (str(client_limit), now_timestamp)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(json_data, f, indent=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encode_all_data_and_dump_json(data_list, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unique check (TOKYO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "for index in range(1):\n",
    "    for i in tqdm(range(24)):\n",
    "        files = glob.glob(\"/Users/fumiyuki/Downloads/time-tokyo-%s/*\" % str(i+1))\n",
    "        for file_name in files:\n",
    "            print(file_name)\n",
    "            time_tokyo = pd.read_csv(file_name, header=None)\n",
    "            time_tokyo = time_tokyo.query('index %% 10 == %s' % index)\n",
    "            time_tokyo = time_tokyo.iloc[:, 3:6]\n",
    "            time_tokyo.columns = [\"time\", \"long\", \"lat\"]\n",
    "            data = pd.concat([data, time_tokyo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_data = data.drop_duplicates()\n",
    "len(dropped_data)/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unique check (KINKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "for index in range(1):\n",
    "    for i in tqdm(range(24)):\n",
    "        files = glob.glob(\"/Users/fumiyuki/Downloads/time-kinki-%s/*\" % str(i+1))\n",
    "        for file_name in files:\n",
    "            print(file_name)\n",
    "            time_tokyo = pd.read_csv(file_name, header=None)\n",
    "            time_tokyo = time_tokyo.query('index %% 10 == %s' % index)\n",
    "            time_tokyo = time_tokyo.iloc[:, 3:6]\n",
    "            time_tokyo.columns = [\"time\", \"long\", \"lat\"]\n",
    "            data = pd.concat([data, time_tokyo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_data = data.drop_duplicates()\n",
    "len(dropped_data)/len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
